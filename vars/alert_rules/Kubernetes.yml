---
prometheus_kubernetes_alert_rules:
# Kubernetes container orchestration system
  - alert: KubePodCrashLooping
    annotations:
      description: Pod is restarting {{ printf "%.0f" $value }} times / 5 minutes.
    expr: "rate(kube_pod_container_status_restarts_total[15m]) * 60 * 5 > 0"
    for: 60m
    labels:
      severity: << vars['prometheus_alert_incident_' + prometheus_alert_incident_levels_map].high >>

  - alert: KubePodNotReady
    annotations:
      description: Pod has been in a non-ready state.
    expr: 'sum by (namespace,pod,datacenter,customer,environment,project,group,instance,service<< tags_key | default("") >>) (max by(namespace,pod,datacenter,customer,environment,project,group,instance,service<< tags_key | default("") >>) (kube_pod_status_phase{phase=~"Pending|Unknown"}) * on(namespace,pod,datacenter,customer,environment,project,group,instance,service<< tags_key | default("") >>) group_left(owner_kind) max by(namespace,pod,owner_kind,datacenter,customer,environment,project,group,instance,service<< tags_key | default("") >>) (kube_pod_owner{owner_kind!="Job"})) > 0'
    for: 60m
    labels:
      severity: << vars['prometheus_alert_incident_' + prometheus_alert_incident_levels_map].high >>

  - alert: KubeContainerWaiting
    annotations:
      description: Pod container waiting longer than 1 hour.
    expr: 'sum by (namespace,pod,containerdatacenter,customer,environment,project,group,instance,service<< tags_key | default("") >>) (kube_pod_container_status_waiting_reason) > 0'
    for: 60m
    labels:
      severity: << vars['prometheus_alert_incident_' + prometheus_alert_incident_levels_map].high >>

  - alert: KubeDeploymentReplicasMismatch
    annotations:
      description: Deployment has not matched the expected number of replicas for longer than an hour.
    expr: "kube_deployment_spec_replicas != kube_deployment_status_replicas_available"
    for: 60m
    labels:
      severity: << vars['prometheus_alert_incident_' + prometheus_alert_incident_levels_map].high >>

  - alert: KubeDeploymentGenerationMismatch
    annotations:
      description: Deployment generation does not match, this indicates that the Deployment has failed but has not been rolled back.
    expr: "kube_deployment_status_observed_generation != kube_deployment_metadata_generation"
    for: 15m
    labels:
      severity: << vars['prometheus_alert_incident_' + prometheus_alert_incident_levels_map].high >>

  - alert: KubeStatefulSetReplicasMismatch
    annotations:
      description: StatefulSet has not matched the expected number of replicas for longer than 30 minutes.
    expr: "kube_statefulset_status_replicas_ready != kube_statefulset_status_replicas"
    for: 30m
    labels:
      severity: << vars['prometheus_alert_incident_' + prometheus_alert_incident_levels_map].high >>

  - alert: KubeStatefulSetGenerationMismatch
    annotations:
      description: StatefulSet generation does not match, this indicates that the StatefulSet has failed but has not been rolled back.
    expr: "kube_statefulset_status_observed_generation != kube_statefulset_metadata_generation"
    for: 30m
    labels:
      severity: << vars['prometheus_alert_incident_' + prometheus_alert_incident_levels_map].high >>
  
  - alert: KubeStatefulSetUpdateNotRolledOut
    annotations:
      description: StatefulSet update has not been rolled out.
    expr: "max without (revision) ( kube_statefulset_status_current_revision unless kube_statefulset_status_update_revision ) * ( kube_statefulset_replicas != kube_statefulset_status_replicas_updated )"
    for: 30m
    labels:
      severity: << vars['prometheus_alert_incident_' + prometheus_alert_incident_levels_map].high >>
  
  - alert: KubeDaemonSetRolloutStuck
    annotations:
      description: Only {{ printf "%0.0f" $value }}% of the desired Pods of DaemonSet are scheduled and ready.
    expr: "kube_daemonset_status_number_ready / kube_daemonset_status_desired_number_scheduled * 100 < 100"
    for: 15m
    labels:
      severity: << vars['prometheus_alert_incident_' + prometheus_alert_incident_levels_map].high >>
  
  - alert: KubeDaemonSetNotScheduled
    annotations:
      description: '{{ $value }} Pods of DaemonSet are not scheduled.'
    expr: "kube_daemonset_status_desired_number_scheduled - kube_daemonset_status_current_number_scheduled > 0"
    for: 10m
    labels:
      severity: << vars['prometheus_alert_incident_' + prometheus_alert_incident_levels_map].high >>
  
  - alert: KubeDaemonSetMisScheduled
    annotations:
      description: '{{ $value }} Pods of DaemonSet are running where they are not supposed to run.'
    expr: "kube_daemonset_status_number_misscheduled > 0"
    for: 10m
    labels:
      severity: << vars['prometheus_alert_incident_' + prometheus_alert_incident_levels_map].high >>
  
  - alert: KubeCronJobRunning
    annotations:
      description: CronJob is taking more than 1h to complete.
    expr: "time() - kube_cronjob_next_schedule_time > 3600"
    for: 60m
    labels:
      severity: << vars['prometheus_alert_incident_' + prometheus_alert_incident_levels_map].high >>
  
  - alert: KubeJobCompletion
    annotations:
      description: Job is taking more than one hour to complete.
    expr: "kube_job_spec_completions - kube_job_status_succeeded > 0"
    for: 60m
    labels:
      severity: << vars['prometheus_alert_incident_' + prometheus_alert_incident_levels_map].high >>
  
  - alert: KubeJobFailed
    annotations:
      description: Job failed to complete.
    expr: "kube_job_status_failed > 0"
    for: 60m
    labels:
      severity: << vars['prometheus_alert_incident_' + prometheus_alert_incident_levels_map].high >>

  - alert: KubeNodeNotReady
    annotations:
      description: 'Node has been unready.'
    expr: 'kube_node_status_condition{condition="Ready",status="true"} == 0'
    for: 10m
    labels:
      severity: << vars['prometheus_alert_incident_' + prometheus_alert_incident_levels_map].critical >>
  
  - alert: KubeVersionMismatch
    annotations:
      description: There are {{ $value }} different semantic versions of Kubernetes components running.
    expr: 'count by (gitVersion,customer,environment,project,group)(count by (gitVersion,customer,environment,project,group) (label_replace(kubernetes_build_info,"gitVersion","$1","gitVersion","(v[0-9]*.[0-9]*.[0-9]*).*"))) > 1'
    for: 60m
    labels:
      severity: << vars['prometheus_alert_incident_' + prometheus_alert_incident_levels_map].high >>
  
  - alert: KubeClientErrors
    annotations:
      description: Kubernetes API server client is experiencing {{ printf "%0.0f" $value }}% errors.'
    expr: '(sum(rate(rest_client_requests_total{code=~"5.."}[5m])) by (datacenter,customer,environment,project,group,instance,service<< tags_key | default("") >>) / sum(rate(rest_client_requests_total[5m])) by (datacenter,customer,environment,project,group,instance,service<< tags_key | default("") >>)) * 100 > 1'
    for: 15m
    labels:
      severity: << vars['prometheus_alert_incident_' + prometheus_alert_incident_levels_map].high >>
  
  - alert: KubeletTooManyPods
    annotations:
      description: Kubelet is running {{ $value }} Pods.
    expr: "kubelet_running_pod_count > 110 * 0.9"
    for: 15m
    labels:
      severity: << vars['prometheus_alert_incident_' + prometheus_alert_incident_levels_map].critical >>
  
  - alert: KubeClientCertificateExpiration
    annotations:
      description: A client certificate used to authenticate to the apiserver is expiring in less than 7 days.
    expr: "apiserver_client_certificate_expiration_seconds_count > 0 and histogram_quantile(0.01, sum by (le,datacenter,customer,environment,project,group,instance,service<< tags_key | default('') >>) (rate(apiserver_client_certificate_expiration_seconds_bucket[5m]))) < 604800"
    labels:
      severity: << vars['prometheus_alert_incident_' + prometheus_alert_incident_levels_map].high >>
  
  - alert: KubeClientCertificateExpiration
    annotations:
      description: A client certificate used to authenticate to the apiserver is expiring in less than 24 hours.
    expr: "apiserver_client_certificate_expiration_seconds_count > 0 and histogram_quantile(0.01, sum by (le,datacenter,customer,environment,project,group,instance,service<< tags_key | default('') >>) (rate(apiserver_client_certificate_expiration_seconds_bucket[5m]))) < 86400"
    labels:
      severity: << vars['prometheus_alert_incident_' + prometheus_alert_incident_levels_map].critical >>

  - alert: KubePersistentVolumeErrors
    annotations:
      description: The persistent volume status has fall.
    expr: 'kube_persistentvolume_status_phase{phase=~"Failed|Pending"} > 0'
    for: 5m
    labels:
      severity: << vars['prometheus_alert_incident_' + prometheus_alert_incident_levels_map].critical >>

  - alert: HPAScalingAbility
    annotations:
      description: 'HPA is suffering from a problem preventing scaling from occurring.'
    expr: 'kube_hpa_status_condition{condition="false", status="AbleToScale"} == 1'
    for: 30m
    labels:
      severity: << vars['prometheus_alert_incident_' + prometheus_alert_incident_levels_map].critical >>
 
  - alert: HPAMetricsAvailability
    annotations:
      description: 'HPA is not able to collect metrics.'
    expr: 'kube_hpa_status_condition{condition="false", status="ScalingActive"} == 1'
    for: 30m
    labels:
      severity: << vars['prometheus_alert_incident_' + prometheus_alert_incident_levels_map].critical >>
 
  - alert: HPAScaleCapability
    annotations:
      description: 'HPA has hit the maximum number of desired Pods.'
    expr: 'sum(kube_hpa_status_desired_replicas) by (hpa,datacenter,customer,environment,project,group,instance,service<< tags_key | default("") >>) >= sum(kube_hpa_spec_max_replicas) by (hpa,datacenter,customer,environment,project,group,instance,service<< tags_key | default("") >>)'
    for: 30m
    labels:
      severity: << vars['prometheus_alert_incident_' + prometheus_alert_incident_levels_map].critical >>

  - alert: KubeHpaReplicasMismatch
    annotations:
      description: 'HPA has not matched descired number of replicas.'
    expr: '(kube_hpa_status_desired_replicas != kube_hpa_status_current_replicas) and changes(kube_hpa_status_current_replicas[15m]) == 0'
    for: 30m
    labels:
      severity: << vars['prometheus_alert_incident_' + prometheus_alert_incident_levels_map].high >>

  - alert: KubeHpaMaxedOut
    annotations:
      description: 'HPA is running at max replicas.'
    expr: 'kube_hpa_status_current_replicas == kube_hpa_spec_max_replicas'
    for: 30m
    labels:
      severity: << vars['prometheus_alert_incident_' + prometheus_alert_incident_levels_map].warning >>

  - alert: KubeCPUOvercommit
    annotations:
      description: 'Cluster has overcommitted CPU resource requests.'
    expr: 'sum(sum(kube_pod_container_resource_requests_cpu_cores and on(pod) kube_pod_status_scheduled{condition="true"})) / sum(kube_node_status_allocatable_cpu_cores) > (count(kube_node_status_allocatable_cpu_cores)-1) / count(kube_node_status_allocatable_cpu_cores)'
    for: 30m
    labels:
      severity: << vars['prometheus_alert_incident_' + prometheus_alert_incident_levels_map].warning >>

  - alert: KubeCPUQuotaOvercommit
    annotations:
      description: 'Cluster has overcommitted CPU resource quota.'
    expr: 'sum(kube_resourcequota{type="hard", resource="cpu"}) / sum(kube_node_status_allocatable_cpu_cores) > 1.5'
    for: 30m
    labels:
      severity: << vars['prometheus_alert_incident_' + prometheus_alert_incident_levels_map].warning >>

  - alert: KubeMemoryQuotaOvercommit
    annotations:
      description: 'Cluster has overcommitted memory resource quota.'
    expr: 'sum(kube_resourcequota{type="hard", resource="memory"}) / sum(kube_node_status_allocatable_memory_bytes{job="node-exporter"}) > 1.5'
    for: 30m
    labels:
      severity: << vars['prometheus_alert_incident_' + prometheus_alert_incident_levels_map].warning >>
